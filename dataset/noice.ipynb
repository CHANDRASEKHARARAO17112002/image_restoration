{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "38/38 [==============================] - 248s 6s/step - loss: 0.0513\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 238s 6s/step - loss: 0.0166\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 219s 6s/step - loss: 0.0146\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 204s 5s/step - loss: 0.0133\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 204s 5s/step - loss: 0.0141\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 200s 5s/step - loss: 0.0138\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 204s 5s/step - loss: 0.0135\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 184s 5s/step - loss: 0.0131\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 183s 5s/step - loss: 0.0135\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 191s 5s/step - loss: 0.0132\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Epoch 1/10, Generator Loss: 0.7170, Discriminator Loss: 0.7048\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Epoch 2/10, Generator Loss: 0.6968, Discriminator Loss: 0.7036\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Epoch 3/10, Generator Loss: 0.6621, Discriminator Loss: 0.7008\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Epoch 4/10, Generator Loss: 0.6258, Discriminator Loss: 0.6999\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Epoch 5/10, Generator Loss: 0.5882, Discriminator Loss: 0.6964\n",
      "1/1 [==============================] - 1s 996ms/step\n",
      "Epoch 6/10, Generator Loss: 0.5878, Discriminator Loss: 0.7161\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Epoch 7/10, Generator Loss: 0.5376, Discriminator Loss: 0.7042\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Epoch 8/10, Generator Loss: 0.5341, Discriminator Loss: 0.7192\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Epoch 9/10, Generator Loss: 0.5465, Discriminator Loss: 0.7372\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Epoch 10/10, Generator Loss: 0.4967, Discriminator Loss: 0.7210\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Conv2D, LeakyReLU, UpSampling2D, Input, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================== #\n",
    "#        CONFIGURATIONS          #\n",
    "# ============================== #\n",
    "IMG_HEIGHT, IMG_WIDTH = 256, 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "\n",
    "# Paths\n",
    "ground_truth_dir = r\"E:\\dataset\\archive (1)\\Ground_truth\"\n",
    "noisy_dir = r\"E:\\dataset\\archive (1)\\Noisy_folder\"\n",
    "\n",
    "# ============================== #\n",
    "#        DATA LOADING            #\n",
    "# ============================== #\n",
    "def load_images(path):\n",
    "    images = []\n",
    "    for file in os.listdir(path):\n",
    "        img = cv2.imread(os.path.join(path, file))\n",
    "        img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        img = img / 255.0  # Normalize\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "X_train_noisy = load_images(noisy_dir)\n",
    "X_train_clean = load_images(ground_truth_dir)\n",
    "\n",
    "# ============================== #\n",
    "#        CNN DENOISER            #\n",
    "# ============================== #\n",
    "def build_cnn():\n",
    "    inputs = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    x = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = Conv2D(3, (3, 3), padding=\"same\", activation=\"sigmoid\")(x)  # Ensure same shape\n",
    "    return Model(inputs, x)\n",
    "\n",
    "cnn = build_cnn()\n",
    "cnn.compile(optimizer=Adam(learning_rate=0.001), loss=MeanSquaredError())\n",
    "\n",
    "# Train CNN\n",
    "cnn.fit(X_train_noisy, X_train_clean, epochs=10, batch_size=8)\n",
    "cnn.save(\"cnn_denoising_model.h5\")\n",
    "\n",
    "# ============================== #\n",
    "#        GAN MODELS              #\n",
    "# ============================== #\n",
    "def build_generator():\n",
    "    inputs = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = Conv2D(3, (3, 3), padding=\"same\", activation=\"sigmoid\")(x)  # Ensure same shape\n",
    "    \n",
    "    return Model(inputs, x)\n",
    "\n",
    "def build_discriminator():\n",
    "    inputs = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), padding=\"same\")(inputs)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(128, (3, 3), padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(1, (3, 3), padding=\"same\", activation=\"sigmoid\")(x)\n",
    "    \n",
    "    return Model(inputs, x)\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "discriminator.compile(optimizer=Adam(0.0002), loss=BinaryCrossentropy())\n",
    "\n",
    "# ============================== #\n",
    "#        GAN TRAINING            #\n",
    "# ============================== #\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    idx = np.random.randint(0, X_train_noisy.shape[0], BATCH_SIZE)\n",
    "    noisy_images = X_train_noisy[idx]\n",
    "    clean_images = X_train_clean[idx]\n",
    "    \n",
    "    generated_images = generator.predict(noisy_images)\n",
    "    \n",
    "    real_labels = np.ones((BATCH_SIZE, IMG_HEIGHT, IMG_WIDTH, 1))\n",
    "    fake_labels = np.zeros((BATCH_SIZE, IMG_HEIGHT, IMG_WIDTH, 1))\n",
    "    \n",
    "    # Train discriminator\n",
    "    d_loss_real = discriminator.train_on_batch(clean_images, real_labels)\n",
    "    d_loss_fake = discriminator.train_on_batch(generated_images, fake_labels)\n",
    "    d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "    \n",
    "    # Train generator\n",
    "    g_loss = discriminator.train_on_batch(noisy_images, real_labels)\n",
    "\n",
    "    # Store losses\n",
    "    g_losses.append(g_loss)\n",
    "    d_losses.append(d_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Generator Loss: {g_loss:.4f}, Discriminator Loss: {d_loss:.4f}\")\n",
    "\n",
    "# Save models\n",
    "generator.save(\"gan_generator.h5\")\n",
    "discriminator.save(\"gan_discriminator.h5\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 215ms/step\n",
      "Denoised image saved as denoised_output.png\n"
     ]
    }
   ],
   "source": [
    "# ============================== #\n",
    "#        TEST DENOISING          #\n",
    "# ============================== #\n",
    "def test_denoising(image_path):\n",
    "    img = cv2.imread(image_path) / 255.0\n",
    "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "    denoised_img = generator.predict(img)[0]  # Remove batch dim\n",
    "    denoised_img = (denoised_img * 255).astype(np.uint8)\n",
    "\n",
    "    cv2.imwrite(\"denoised_output.png\", denoised_img)\n",
    "    print(\"Denoised image saved as denoised_output.png\")\n",
    "\n",
    "test_denoising(r\"E:/dataset/archive (1)/Noisy_folder/noisy_144839343_db2326a111_c.jpg\")\n",
    "\n",
    "# ============================== #\n",
    "#        SALT-PEPPER FILTER      #\n",
    "# ============================== #\n",
    "def remove_salt_pepper_noise(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    denoised_img = cv2.medianBlur(img, 3)  # Apply median filter\n",
    "    cv2.imwrite(\"denoised_median.png\", denoised_img)\n",
    "\n",
    "remove_salt_pepper_noise(r\"E:\\dataset\\archive (1)\\Noisy_folder\\noisy_144839343_db2326a111_c.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"reshape_1\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [16, 16, 32768], output_shape = [16, 16, 128]\n\nCall arguments received by layer \"reshape_1\" (type Reshape):\n  â€¢ inputs=tf.Tensor(shape=(None, 16, 16, 32768), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 79\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gan\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Compile and build the models\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m build_discriminator()\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Compile Discriminator\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 45\u001b[0m, in \u001b[0;36mbuild_generator\u001b[1;34m(input_shape)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_generator\u001b[39m(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m1\u001b[39m)):  \u001b[38;5;66;03m# Latent space input\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Latent input\u001b[39;49;00m\n\u001b[0;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mReshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mUpSampling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msame\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mUpSampling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msame\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msigmoid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msame\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Output shape (128, 128, 3)\u001b[39;49;00m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\tech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\tech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\reshaping\\reshape.py:118\u001b[0m, in \u001b[0;36mReshape._fix_unknown_dimension\u001b[1;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[0;32m    116\u001b[0m     output_shape[unknown] \u001b[38;5;241m=\u001b[39m original \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m known\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original \u001b[38;5;241m!=\u001b[39m known:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_shape\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"reshape_1\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [16, 16, 32768], output_shape = [16, 16, 128]\n\nCall arguments received by layer \"reshape_1\" (type Reshape):\n  â€¢ inputs=tf.Tensor(shape=(None, 16, 16, 32768), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, LeakyReLU, BatchNormalization, UpSampling2D, Dense, Flatten, Reshape\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to load and preprocess images (with salt-and-pepper noise)\n",
    "def load_and_preprocess_images(image_dir, img_size=(128, 128)):\n",
    "    images = []\n",
    "    for filename in os.listdir(image_dir):\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue  # Skip files that can't be read\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, img_size)\n",
    "        img = cv2.medianBlur(img, 3)  # Apply median filtering to remove salt-and-pepper noise\n",
    "        img = img / 255.0  # Normalize\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# Load noisy and normal (clean) images\n",
    "low_light_images = load_and_preprocess_images(r\"E:\\dataset\\archive (1)\\Noisy_folder\")\n",
    "normal_images = load_and_preprocess_images(r\"E:\\dataset\\archive (1)\\Ground_truth\")\n",
    "\n",
    "# CNN-based Feature Extractor\n",
    "feature_extractor = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(128, 128, 3)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Reshape((16, 16, 1)),\n",
    "])\n",
    "\n",
    "# Generator Model\n",
    "def build_generator(input_shape=(16, 16, 1)):  # Latent space input\n",
    "    model = Sequential([\n",
    "        Dense(128 * 16 * 16, activation='relu', input_shape=input_shape),  # Latent input\n",
    "        Reshape((16, 16, 128)),\n",
    "        UpSampling2D(),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        UpSampling2D(),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(3, (3, 3), activation='sigmoid', padding='same')  # Output shape (128, 128, 3)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Discriminator Model\n",
    "def build_discriminator(input_shape=(128, 128, 3)):  # Accepts generated image\n",
    "    model = Sequential([\n",
    "        Conv2D(64, (3, 3), padding='same', input_shape=input_shape),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Conv2D(128, (3, 3), padding='same'),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Flatten(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# GAN Model\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False  # Freeze discriminator during GAN training\n",
    "    z = tf.keras.Input(shape=(16, 16, 1))  # Latent input shape (16, 16, 1)\n",
    "    generated_img = generator(z)\n",
    "    validity = discriminator(generated_img)\n",
    "    gan = Model(z, validity)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "    return gan\n",
    "\n",
    "# Compile and build the models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Compile Discriminator\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "# Build GAN\n",
    "gan = build_gan(generator, discriminator)\n",
    "\n",
    "# Training Loop\n",
    "def train_gan(epochs=50, batch_size=32):\n",
    "    for epoch in range(epochs):\n",
    "        idx = np.random.randint(0, low_light_images.shape[0], batch_size)\n",
    "        imgs_low_light = low_light_images[idx]\n",
    "        imgs_normal = normal_images[idx]\n",
    "        \n",
    "        # Get features from low-light images using the feature extractor\n",
    "        feature_input = feature_extractor.predict(imgs_low_light)\n",
    "        \n",
    "        # Generate images using the generator\n",
    "        gen_imgs = generator.predict(feature_input)\n",
    "        \n",
    "        # Train discriminator on real and generated images\n",
    "        d_loss_real, d_acc_real = discriminator.train_on_batch(imgs_normal, np.ones((batch_size, 1)))\n",
    "        d_loss_fake, d_acc_fake = discriminator.train_on_batch(gen_imgs, np.zeros((batch_size, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        d_acc = 0.5 * np.add(d_acc_real, d_acc_fake)\n",
    "        \n",
    "        # Train GAN on the latent input, pushing the generator to create real-looking images\n",
    "        g_loss = gan.train_on_batch(feature_input, np.ones((batch_size, 1)))\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}: D Loss={d_loss:.4f}, D Accuracy={d_acc:.4f}, G Loss={g_loss:.4f}\")\n",
    "\n",
    "train_gan()\n",
    "\n",
    "# Save the generator model (low-light enhancer)\n",
    "generator.save(\"low_light_enhancer.h5\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 128, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name='conv2d_48_input'), name='conv2d_48_input', description=\"created by layer 'conv2d_48_input'\"), but it was called on an input with incompatible shape (None, 512, 512, 3).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"sequential_13\" (type Sequential).\n\nInput 0 of layer \"dense_9\" is incompatible with the layer: expected axis -1 of input shape to have value 2097152, but received input with shape (None, 33554432)\n\nCall arguments received by layer \"sequential_13\" (type Sequential):\n  â€¢ inputs=tf.Tensor(shape=(None, 512, 512, 3), dtype=float32)\n  â€¢ training=False\n  â€¢ mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m discriminator\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39mAdam(\u001b[38;5;241m0.0002\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Build GAN\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m gan \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Training Loop with accuracy printing\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_gan\u001b[39m(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n",
      "Cell \u001b[1;32mIn[7], line 63\u001b[0m, in \u001b[0;36mbuild_gan\u001b[1;34m(generator, discriminator)\u001b[0m\n\u001b[0;32m     61\u001b[0m z \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m3\u001b[39m))  \u001b[38;5;66;03m# Input noisy image (128, 128, 3)\u001b[39;00m\n\u001b[0;32m     62\u001b[0m generated_img \u001b[38;5;241m=\u001b[39m generator(z)\n\u001b[1;32m---> 63\u001b[0m validity \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m gan \u001b[38;5;241m=\u001b[39m Model(z, validity)\n\u001b[0;32m     65\u001b[0m gan\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39mAdam(\u001b[38;5;241m0.0002\u001b[39m, \u001b[38;5;241m0.5\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\tech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\tech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\input_spec.py:277\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    272\u001b[0m             value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape_as_list[\u001b[38;5;28mint\u001b[39m(axis)] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    274\u001b[0m             value,\n\u001b[0;32m    275\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    276\u001b[0m         }:\n\u001b[1;32m--> 277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    278\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    279\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_shape(x\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m             )\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"sequential_13\" (type Sequential).\n\nInput 0 of layer \"dense_9\" is incompatible with the layer: expected axis -1 of input shape to have value 2097152, but received input with shape (None, 33554432)\n\nCall arguments received by layer \"sequential_13\" (type Sequential):\n  â€¢ inputs=tf.Tensor(shape=(None, 512, 512, 3), dtype=float32)\n  â€¢ training=False\n  â€¢ mask=None"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, LeakyReLU, BatchNormalization, UpSampling2D, Dense, Flatten\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to load and preprocess images (resize to 128x128)\n",
    "def load_and_preprocess_images(image_dir, img_size=(128, 128)):\n",
    "    images = []\n",
    "    for filename in os.listdir(image_dir):\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue  # Skip files that can't be read\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, img_size)  # Ensure the image is resized to (128, 128)\n",
    "        img = cv2.medianBlur(img, 3)  # Apply median filtering to remove salt-and-pepper noise\n",
    "        img = img / 255.0  # Normalize\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# Load noisy and normal (clean) images\n",
    "low_light_images = load_and_preprocess_images(r\"E:\\dataset\\archive (1)\\Noisy_folder\")\n",
    "normal_images = load_and_preprocess_images(r\"E:\\dataset\\archive (1)\\Ground_truth\")\n",
    "\n",
    "# Generator Model\n",
    "def build_generator(input_shape=(128, 128, 3)):  # Input is a noisy image (128, 128, 3)\n",
    "    model = Sequential([\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        UpSampling2D(),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        UpSampling2D(),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(3, (3, 3), activation='sigmoid', padding='same')  # Output clean image\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Discriminator Model\n",
    "def build_discriminator(input_shape=(128, 128, 3)):  # Accepts generated image\n",
    "    model = Sequential([\n",
    "        Conv2D(64, (3, 3), padding='same', input_shape=input_shape),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Conv2D(128, (3, 3), padding='same'),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Flatten(),\n",
    "        Dense(1, activation='sigmoid')  # Binary classification (real or fake)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# GAN Model\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False  # Freeze discriminator during GAN training\n",
    "    z = tf.keras.Input(shape=(128, 128, 3))  # Input noisy image (128, 128, 3)\n",
    "    generated_img = generator(z)\n",
    "    validity = discriminator(generated_img)\n",
    "    gan = Model(z, validity)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "    return gan\n",
    "\n",
    "# Compile and build the models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Compile Discriminator\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "# Build GAN\n",
    "gan = build_gan(generator, discriminator)\n",
    "\n",
    "# Training Loop with accuracy printing\n",
    "def train_gan(epochs=50, batch_size=32):\n",
    "    for epoch in range(epochs):\n",
    "        idx = np.random.randint(0, low_light_images.shape[0], batch_size)\n",
    "        imgs_low_light = low_light_images[idx]\n",
    "        imgs_normal = normal_images[idx]\n",
    "        \n",
    "        # Train discriminator on real and generated images\n",
    "        d_loss_real, d_acc_real = discriminator.train_on_batch(imgs_normal, np.ones((batch_size, 1)))\n",
    "        gen_imgs = generator.predict(imgs_low_light)\n",
    "        d_loss_fake, d_acc_fake = discriminator.train_on_batch(gen_imgs, np.zeros((batch_size, 1)))\n",
    "        \n",
    "        # Average discriminator loss and accuracy\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        d_acc = 0.5 * np.add(d_acc_real, d_acc_fake)\n",
    "        \n",
    "        # Train GAN on the noisy images to improve generator\n",
    "        g_loss = gan.train_on_batch(imgs_low_light, np.ones((batch_size, 1)))  # The generator aims to fool the discriminator\n",
    "        \n",
    "        # Print discriminator and generator metrics\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: D Loss={d_loss:.4f}, D Accuracy={d_acc:.4f}, G Loss={g_loss:.4f}\")\n",
    "        print(f\"Discriminator - Real Accuracy: {d_acc_real:.4f}, Fake Accuracy: {d_acc_fake:.4f}\")\n",
    "\n",
    "train_gan()\n",
    "\n",
    "# Save the generator model (low-light enhancer)\n",
    "generator.save(\"low_light_enhancer.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Add noise\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m gaussian_noisy_image \u001b[38;5;241m=\u001b[39m \u001b[43madd_gaussian_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m salt_pepper_noisy_image \u001b[38;5;241m=\u001b[39m add_salt_pepper_noise(image)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Save noisy images\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m, in \u001b[0;36madd_gaussian_noise\u001b[1;34m(image, mean, var)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_gaussian_noise\u001b[39m(image, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, var\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m):\n\u001b[0;32m      5\u001b[0m     sigma \u001b[38;5;241m=\u001b[39m var\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m----> 6\u001b[0m     gaussian \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(mean, sigma, \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[0;32m      7\u001b[0m     noisy_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(image \u001b[38;5;241m+\u001b[39m gaussian, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m noisy_image\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def add_gaussian_noise(image, mean=0, var=0.01):\n",
    "    sigma = var**0.5\n",
    "    gaussian = np.random.normal(mean, sigma, image.shape)\n",
    "    noisy_image = np.clip(image + gaussian, 0, 255).astype(np.uint8)\n",
    "    return noisy_image\n",
    "\n",
    "def add_salt_pepper_noise(image, prob=0.05):\n",
    "    noisy_image = image.copy()\n",
    "    num_salt = np.ceil(prob * image.size * 0.5)\n",
    "    coords = [np.random.randint(0, i - 1, int(num_salt)) for i in image.shape]\n",
    "    noisy_image[coords] = 255\n",
    "\n",
    "    num_pepper = np.ceil(prob * image.size * 0.5)\n",
    "    coords = [np.random.randint(0, i - 1, int(num_pepper)) for i in image.shape]\n",
    "    noisy_image[coords] = 0\n",
    "    return noisy_image\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread('image.jpg', cv2.IMREAD_COLOR)\n",
    "\n",
    "# Add noise\n",
    "gaussian_noisy_image = add_gaussian_noise(image)\n",
    "salt_pepper_noisy_image = add_salt_pepper_noise(image)\n",
    "\n",
    "# Save noisy images\n",
    "cv2.imwrite('gaussian_noise.jpg', gaussian_noisy_image)\n",
    "cv2.imwrite('salt_pepper_noise.jpg', salt_pepper_noisy_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
